{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "\n",
    "# Tokenized & Lemmatized data\n",
    "#data = pd.read_csv('../Trip Advisor Preprocessing/data_tokenized_Isak.csv') #Smaller dataset\n",
    "data = pd.read_csv('../Trip Advisor Preprocessing/data_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings.overall\n",
       "aNegative     1577\n",
       "bNeutral      1945\n",
       "cPositive    19959\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing all values with sentiment labels as discussed in the paper\n",
    "\n",
    "data['ratings.overall'] = data['ratings.overall'].replace(range(0, 3), 'aNegative')\n",
    "data['ratings.overall'] = data['ratings.overall'].replace(3, 'bNeutral')\n",
    "data['ratings.overall'] = data['ratings.overall'].replace(range(4, 6), 'cPositive')\n",
    "\n",
    "result = data.groupby('ratings.overall').size()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataframing and turning input & outvariable to model-readable list. \n",
    "preprocessed_data = data['text'].tolist()\n",
    "labels = data['ratings.overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train split and subsequent validation / test splits. \n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(preprocessed_data,labels,train_size=0.75,stratify=labels) #75% train data\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5) #12.5% in test and 12.5% in validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'logreg__C': 10.0, 'logreg__max_iter': 10000, 'logreg__solver': 'saga', 'vect__max_df': 0.9, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}\n",
      "Best accuracy score:  0.8773240432671511\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression()),\n",
    "])\n",
    "\n",
    "\n",
    "# Define parameters for grid search\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'vect__max_df': [0.9, 0.95, 1.0],\n",
    "    'vect__min_df': [1, 2, 3],\n",
    "    'logreg__C': [0.1, 1.0, 10.0],\n",
    "    'logreg__max_iter': [10000],\n",
    "    'logreg__solver': ['saga'],\n",
    "}\n",
    "\n",
    "\n",
    "# Create grid search object\n",
    "grid_search = GridSearchCV(pipeline, parameters, scoring='f1_weighted', cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "#grid_search.fit(X_train, y_train)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model used:  Pipeline(steps=[('vect', TfidfVectorizer(max_df=0.9, ngram_range=(1, 2))),\n",
      "                ('logreg',\n",
      "                 LogisticRegression(C=10.0, max_iter=10000, solver='saga'))])\n",
      "\n",
      "Validation set Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.56      0.63       196\n",
      "     Netural       0.54      0.27      0.36       242\n",
      "    Positive       0.92      0.98      0.95      2497\n",
      "\n",
      "    accuracy                           0.89      2935\n",
      "   macro avg       0.73      0.60      0.65      2935\n",
      "weighted avg       0.88      0.89      0.88      2935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the val data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_val = best_model.predict(X_valid)\n",
    "print(\"Best model used: \", best_model)\n",
    "\n",
    "#target_names = ['Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5']\n",
    "target_names = ['Negative', 'Netural', 'Positive']\n",
    "print('\\nValidation set Classification Report: \\n',classification_report(y_valid, y_pred_val, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model used:  Pipeline(steps=[('vect', TfidfVectorizer(max_df=0.9, ngram_range=(1, 2))),\n",
      "                ('logreg',\n",
      "                 LogisticRegression(C=10.0, max_iter=10000, solver='saga'))])\n",
      "\n",
      "Test set Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.63      0.70       198\n",
      "     Netural       0.48      0.24      0.32       244\n",
      "    Positive       0.92      0.98      0.95      2494\n",
      "\n",
      "    accuracy                           0.89      2936\n",
      "   macro avg       0.73      0.62      0.66      2936\n",
      "weighted avg       0.87      0.89      0.88      2936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FINAL SCORING ON TEST DATA\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best model used: \", best_model)\n",
    "\n",
    "target_names = ['Negative', 'Netural', 'Positive']\n",
    "print('\\nTest set Classification Report: \\n',classification_report(y_test, y_pred_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 keywords for a Positive rating:\n",
      "1. great: 9.119\n",
      "2. excellent: 7.633\n",
      "3. loved: 6.368\n",
      "4. wonderful: 5.235\n",
      "5. beautiful: 5.099\n",
      "6. perfect: 4.564\n",
      "7. everything: 4.561\n",
      "8. nice: 4.445\n",
      "9. best: 4.274\n",
      "10. fantastic: 4.119\n",
      "\n",
      "Top 10 keywords for a Neutral rating:\n",
      "1. average: 4.735\n",
      "2. okay: 4.474\n",
      "3. ok: 4.310\n",
      "4. basic: 3.583\n",
      "5. location stay: 3.484\n",
      "6. nothing: 3.428\n",
      "7. frill: 3.371\n",
      "8. breakfast staff: 3.251\n",
      "9. small: 3.229\n",
      "10. dated: 3.223\n",
      "\n",
      "Top 10 keywords for a Negative rating:\n",
      "1. dirty: 8.305\n",
      "2. rude: 6.376\n",
      "3. worst: 5.773\n",
      "4. horrible: 5.607\n",
      "5. poor: 5.514\n",
      "6. terrible: 5.161\n",
      "7. bug: 4.764\n",
      "8. never: 4.631\n",
      "9. awful: 3.917\n",
      "10. cockroach: 3.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johanisak/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#This part was developed in collaboration with ChatGPT based on the metric coefficient chosen by the project group. \n",
    "\n",
    "# Extract top 10 keywords that have an implication on the model classifying it as 'bad'\n",
    "feature_names = best_model.named_steps['vect'].get_feature_names()\n",
    "top10_bad_idx = best_model.named_steps['logreg'].coef_[0, :].argsort()[::-1][:10]\n",
    "top10_bad_keywords = [(feature_names[i], best_model.named_steps['logreg'].coef_[0, i]) for i in top10_bad_idx]\n",
    "\n",
    "\n",
    "# Extract top 10 keywords that have an implication on the model classifying it as 'neutral'\n",
    "top10_neutral_idx = best_model.named_steps['logreg'].coef_[1, :].argsort()[::-1][:10]\n",
    "top10_neutral_keywords = [(feature_names[i], best_model.named_steps['logreg'].coef_[1, i]) for i in top10_neutral_idx]\n",
    "\n",
    "\n",
    "# Extract top 10 keywords that have an implication on the model classifying it as 'good'\n",
    "top10_good_idx = best_model.named_steps['logreg'].coef_[2, :].argsort()[::-1][:10]\n",
    "top10_good_keywords = [(feature_names[i], best_model.named_steps['logreg'].coef_[2, i]) for i in top10_good_idx]\n",
    "\n",
    "\n",
    "# Display top 10 keywords that have an implication on the model classifying it as a good rating\n",
    "print(\"Top 10 keywords for a Positive rating:\")\n",
    "for i, (keyword, value) in enumerate(top10_good_keywords):\n",
    "    print(f\"{i+1}. {keyword}: {value:.3f}\")\n",
    "\n",
    "# Display top 10 keywords that have an implication on the model classifying it as a neutral rating\n",
    "print(\"\\nTop 10 keywords for a Neutral rating:\")\n",
    "for i, (keyword, value) in enumerate(top10_neutral_keywords):\n",
    "    print(f\"{i+1}. {keyword}: {value:.3f}\")\n",
    "\n",
    "\n",
    "# Display top 10 keywords that have an implication on the model classifying it as a bad rating\n",
    "print(\"\\nTop 10 keywords for a Negative rating:\")\n",
    "for i, (keyword, value) in enumerate(top10_bad_keywords):\n",
    "    print(f\"{i+1}. {keyword}: {value:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING DC DATA ON MODEL\n",
    "\n",
    "The following is a test on the above trained model on data from a city not included in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings.overall\n",
       "1.0     28\n",
       "2.0     27\n",
       "3.0     80\n",
       "4.0    374\n",
       "5.0    679\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the Washington DC only dataset for geographic test on unseen data. \n",
    "data_DC = pd.read_csv('../Trip Advisor Preprocessing/data_tokenized_DC.csv')\n",
    "\n",
    "import_DC = data_DC.groupby('ratings.overall').size()\n",
    "import_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings.overall\n",
       "aNegative      55\n",
       "bNeutral       80\n",
       "cPositive    1053\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing all values with sentiment labels as discussed in the paper\n",
    "data_DC['ratings.overall'] = data_DC['ratings.overall'].replace(range(0, 3), 'aNegative')\n",
    "data_DC['ratings.overall'] = data_DC['ratings.overall'].replace(3, 'bNeutral')\n",
    "data_DC['ratings.overall'] = data_DC['ratings.overall'].replace(range(4, 6), 'cPositive')\n",
    "\n",
    "result_DC = data_DC.groupby('ratings.overall').size()\n",
    "\n",
    "result_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataframing and turning input & outvariable to model-readable list. \n",
    "preprocessed_data_DC = data_DC['text'].tolist()\n",
    "labels_DC = data_DC['ratings.overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model used:  {'logreg__C': 10.0, 'logreg__max_iter': 10000, 'logreg__solver': 'saga', 'vect__max_df': 0.9, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}\n",
      "\n",
      "DC set Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.47      0.57        55\n",
      "     Netural       0.47      0.24      0.32        80\n",
      "    Positive       0.93      0.98      0.96      1053\n",
      "\n",
      "    accuracy                           0.91      1188\n",
      "   macro avg       0.70      0.56      0.61      1188\n",
      "weighted avg       0.89      0.91      0.89      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_DC = best_model.predict(preprocessed_data_DC)\n",
    "\n",
    "print(\"Best model used: \", grid_search.best_params_)\n",
    "\n",
    "target_names_DC = ['Negative', 'Netural', 'Positive']\n",
    "print('\\nDC set Classification Report: \\n',classification_report(labels_DC, y_pred_DC, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
