{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "#Tokenized & Lemmatized data\n",
    "data = pd.read_csv('../Trip Advisor Preprocessing/data_tokenized.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings.overall\n",
       "aNegative     1577\n",
       "bNeutral      1945\n",
       "cPositive    19959\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ratings.overall'] = data['ratings.overall'].replace(range(0, 3), 'aNegative')\n",
    "data['ratings.overall'] = data['ratings.overall'].replace(3, 'bNeutral')\n",
    "data['ratings.overall'] = data['ratings.overall'].replace(range(4, 6), 'cPositive')\n",
    "\n",
    "result = data.groupby('ratings.overall').size()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract preprocessed data and labels\n",
    "preprocessed_data = data['text'].tolist()\n",
    "labels = data['ratings.overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rem, y_train, y_rem = train_test_split(preprocessed_data,labels,train_size=0.75,stratify=labels,random_state=42) #75% train data\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5,random_state=42) #12.5% in test and 12.5% in validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
      "Best parameters:  {'rf__class_weight': 'balanced', 'rf__criterion': 'gini', 'rf__max_depth': 70, 'rf__min_samples_leaf': 5, 'rf__n_estimators': 300, 'vect__max_df': 1.0, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}\n",
      "Best accuracy score:  0.8536480045062792\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# Define parameters for grid search\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "    'vect__max_df': [0.9, 0.95, 1.0],\n",
    "    'vect__min_df': [1, 2, 3],\n",
    "    'rf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'rf__n_estimators': [100, 200, 300],\n",
    "    'rf__max_depth': [60,70], #[10, 20, 30],\n",
    "    'rf__min_samples_leaf':[5],\n",
    "    'rf__class_weight':['balanced'],\n",
    "}\n",
    "\n",
    "# Create grid search object\n",
    "grid_search = GridSearchCV(pipeline, parameters, scoring='f1_weighted', cv=5, n_jobs=-1,verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model used:  {'rf__class_weight': 'balanced', 'rf__criterion': 'gini', 'rf__max_depth': 70, 'rf__min_samples_leaf': 5, 'rf__n_estimators': 300, 'vect__max_df': 1.0, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}\n",
      "\n",
      "Validation set Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.52      0.68      0.59       189\n",
      "     Netural       0.29      0.42      0.34       225\n",
      "    Positive       0.95      0.89      0.92      2521\n",
      "\n",
      "    accuracy                           0.84      2935\n",
      "   macro avg       0.59      0.66      0.62      2935\n",
      "weighted avg       0.87      0.84      0.85      2935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the val data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_val = best_model.predict(X_valid)\n",
    "print(\"Best model used: \", grid_search.best_params_)\n",
    "\n",
    "#target_names = ['Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5']\n",
    "target_names = ['Negative', 'Netural', 'Positive']\n",
    "print('\\nValidation set Classification Report: \\n',classification_report(y_valid, y_pred_val, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model used:  Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
      "                ('rf',\n",
      "                 RandomForestClassifier(class_weight='balanced', max_depth=70,\n",
      "                                        min_samples_leaf=5,\n",
      "                                        n_estimators=300))])\n",
      "\n",
      "Test set Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.53      0.77      0.63       205\n",
      "     Netural       0.32      0.40      0.36       261\n",
      "    Positive       0.95      0.89      0.92      2470\n",
      "\n",
      "    accuracy                           0.84      2936\n",
      "   macro avg       0.60      0.69      0.63      2936\n",
      "weighted avg       0.86      0.84      0.85      2936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FINAL SCORING ON TEST DATA\n",
    "\n",
    "# Evaluate the best model on the val data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best model used: \", best_model)\n",
    "\n",
    "target_names = ['Negative', 'Netural', 'Positive']\n",
    "print('\\nTest set Classification Report: \\n',classification_report(y_test, y_pred_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Extract top 10 keywords that have an implication on the model classifying it as 'bad'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m feature_names \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m'\u001b[39;49m\u001b[39mvect\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mget_feature_names()\n\u001b[1;32m      3\u001b[0m top10_bad_idx \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mrf\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfeature_importances_\u001b[39m.\u001b[39margsort()[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:\u001b[39m10\u001b[39m]\n\u001b[1;32m      4\u001b[0m top10_bad_keywords \u001b[39m=\u001b[39m [(feature_names[i], best_model\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mrf\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfeature_importances_[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m top10_bad_idx]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# Extract top 10 keywords that have an implication on the model classifying it as 'bad'\n",
    "feature_names = best_model.named_steps['vect'].get_feature_names()\n",
    "top10_bad_idx = best_model.named_steps['rf'].feature_importances_.argsort()[::-1][:10]\n",
    "top10_bad_keywords = [(feature_names[i], best_model.named_steps['rf'].feature_importances_[i]) for i in top10_bad_idx]\n",
    "\n",
    "# Extract top 10 keywords that have an implication on the model classifying it as 'neutral'\n",
    "top10_neutral_idx = best_model.named_steps['rf'].feature_importances_.argsort()[::-1][:10]\n",
    "top10_neutral_keywords = [(feature_names[i], best_model.named_steps['rf'].feature_importances_[i]) for i in top10_neutral_idx]\n",
    "\n",
    "# Extract top 10 keywords that have an implication on the model classifying it as 'good'\n",
    "top10_good_idx = best_model.named_steps['rf'].feature_importances_.argsort()[::-1][:10]\n",
    "top10_good_keywords = [(feature_names[i], best_model.named_steps['rf'].feature_importances_[i]) for i in top10_good_idx]\n",
    "\n",
    "# Display top 10 keywords that have an implication on the model classifying it as a 5-star rating\n",
    "print(\"Top 10 keywords for a Positive rating:\")\n",
    "for i, (keyword, importance) in enumerate(top10_good_keywords):\n",
    "    print(f\"{i+1}. {keyword}: {importance:.3f}\")\n",
    "\n",
    "# Display top 10 keywords that have an implication on the model classifying it as a 1-star rating\n",
    "print(\"\\nTop 10 keywords for a Neutral rating:\")\n",
    "for i, (keyword, importance) in enumerate(top10_neutral_keywords):\n",
    "    print(f\"{i+1}. {keyword}: {importance:.3f}\")\n",
    "\n",
    "# Display top 10 keywords that have an implication on the model classifying it as a 1-star rating\n",
    "print(\"\\nTop 10 keywords for a Negative rating:\")\n",
    "for i, (keyword, importance) in enumerate(top10_bad_keywords):\n",
    "    print(f\"{i+1}. {keyword}: {importance:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING DC DATA ON MODEL\n",
    "\n",
    "The following is a test on the above trained model on data from a city not included in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings.overall\n",
       "1.0     28\n",
       "2.0     27\n",
       "3.0     80\n",
       "4.0    374\n",
       "5.0    679\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenized & Lemmatized data\n",
    "#data = pd.read_csv('../Trip Advisor Preprocessing/data_tokenized_Isak.csv') #Smaller dataset\n",
    "data_DC = pd.read_csv('../Trip Advisor Preprocessing/data_tokenized_DC.csv')\n",
    "\n",
    "import_DC = data_DC.groupby('ratings.overall').size()\n",
    "import_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings.overall\n",
       "aNegative      55\n",
       "bNeutral       80\n",
       "cPositive    1053\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_DC['ratings.overall'] = data_DC['ratings.overall'].replace(range(0, 3), 'aNegative')\n",
    "data_DC['ratings.overall'] = data_DC['ratings.overall'].replace(3, 'bNeutral')\n",
    "data_DC['ratings.overall'] = data_DC['ratings.overall'].replace(range(4, 6), 'cPositive')\n",
    "\n",
    "result_DC = data_DC.groupby('ratings.overall').size()\n",
    "\n",
    "result_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract preprocessed data and labels\n",
    "preprocessed_data_DC = data_DC['text'].tolist()\n",
    "labels_DC = data_DC['ratings.overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model used:  {'rf__criterion': 'gini', 'rf__max_depth': 70, 'rf__min_samples_leaf': 5, 'rf__n_estimators': 100, 'vect__max_df': 1.0, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "DC set Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.25      0.40        55\n",
      "     Netural       0.00      0.00      0.00        80\n",
      "    Positive       0.90      1.00      0.95      1053\n",
      "\n",
      "    accuracy                           0.90      1188\n",
      "   macro avg       0.61      0.42      0.45      1188\n",
      "weighted avg       0.84      0.90      0.86      1188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johanisak/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/johanisak/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/johanisak/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred_DC = best_model.predict(preprocessed_data_DC)\n",
    "\n",
    "print(\"Best model used: \", grid_search.best_params_)\n",
    "\n",
    "target_names_DC = ['Negative', 'Netural', 'Positive']\n",
    "print('\\nDC set Classification Report: \\n',classification_report(labels_DC, y_pred_DC, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
